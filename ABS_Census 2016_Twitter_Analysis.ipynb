{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Census 2016 Twitter Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing Census Tweets Collection Size: 1370\n",
      "\n",
      "{'text': u\"RT @veritygorman: .@ABSCensus officials will visit Robinvale today ahead of next month's Census. There have been concerns about under repor\\u2026\", 'place_countrycode': ['no_geo'], 'place_name': ['no_geo'], 'user_screen_name': u'ABCMilduraSwanH', 'ent_user_mention': [{u'id': 288610842, u'indices': [3, 16], u'id_str': u'288610842', u'screen_name': u'veritygorman', u'name': u'Verity Gorman'}, {u'id': 172532664, u'indices': [19, 29], u'id_str': u'172532664', u'screen_name': u'ABSCensus', u'name': u'Census Australia'}], 'coordinates': None, 'id_str': u'755170555892948993', 'retweets': 0, 'user_friend': 2435, 'text_clean': 'veritygorman officials visit robinvale today ahead next months census concerns repor', 'user_follower': 3298, 'user': u'ABC Mildura SwanHill', 'favorites': 0, 'user_tweets': 101, 'place_country': ['no_geo'], 'ent_hashtag': [], 'text_token': [u'veritygorman', u'abscensu', u'offici', u'visit', u'robinval', u'today', u'ahead', u'next', u'month', u'censu', u'concern', u'repor'], 'created_at': u'Mon Jul 18 22:41:05 +0000 2016', 'user_location': u'Mildura, Australia', 'place_type': ['no_geo'], 'user_statuses': 3730, '_id': ObjectId('578d5b08bd526e0391e0e323')}\n",
      "{'text': u'Here\\u2019s why the census requires more questions about seniors in long-term care | Ottawa Citizen https://t.co/e2mZda8sRJ', 'place_countrycode': ['no_geo'], 'place_name': ['no_geo'], 'user_screen_name': u'BCC4PC', 'ent_user_mention': [], 'coordinates': None, 'id_str': u'755170875058585602', 'retweets': 0, 'user_friend': 1892, 'text_clean': 'heres census requires questions seniors longterm care  ottawa citizen ', 'user_follower': 2350, 'user': u'BCC Palliative Care', 'favorites': 0, 'user_tweets': 88, 'place_country': ['no_geo'], 'ent_hashtag': [], 'text_token': [u'censu', u'requir', u'question', u'senior', u'longterm', u'care', u'ottawa', u'citizen'], 'created_at': u'Mon Jul 18 22:42:21 +0000 2016', 'user_location': u'New Westminster, BC', 'place_type': ['no_geo'], 'user_statuses': 7060, '_id': ObjectId('578d5b4fbd526e0391e0e324')}\n",
      "{'text': u'RT @mikelayestaran: Todos con las purgas de #Erdogan a vueltas, y \\xbfEgipto? lleva purgando con el visto bueno de Occidente desde 2013 https:\\u2026', 'place_countrycode': ['no_geo'], 'place_name': ['no_geo'], 'user_screen_name': u'IbonBlasco', 'ent_user_mention': [{u'id': 50908485, u'indices': [3, 18], u'id_str': u'50908485', u'screen_name': u'mikelayestaran', u'name': u'mikel ayestaran'}], 'coordinates': None, 'id_str': u'755171414483828736', 'retweets': 0, 'user_friend': 247, 'text_clean': 'mikelayestaran todos con las purgas de erdogan vueltas egipto lleva purgando con el visto bueno de occidente desde  https', 'user_follower': 224, 'user': u'vergUEnza', 'favorites': 0, 'user_tweets': 5, 'place_country': ['no_geo'], 'ent_hashtag': [{u'indices': [44, 52], u'text': u'Erdogan'}], 'text_token': [u'mikelayestaran', u'todo', u'con', u'purga', u'erdogan', u'vuelta', u'egipto', u'lleva', u'purgando', u'con', u'visto', u'bueno', u'occident', u'desd'], 'created_at': u'Mon Jul 18 22:44:30 +0000 2016', 'user_location': u'suelo ', 'place_type': ['no_geo'], 'user_statuses': 8835, '_id': ObjectId('578d5bcfbd526e0391e0e326')}\n",
      "{'text': u'RT @joshua_landis: True. in 1920, all Syr coastal cities Sunni/Xtian. No Syrian town shared by Sunnis &amp; Alawis in first Fr census. https://\\u2026', 'place_countrycode': ['no_geo'], 'place_name': ['no_geo'], 'user_screen_name': u'2ndNewMoon', 'ent_user_mention': [{u'id': 569446928, u'indices': [3, 17], u'id_str': u'569446928', u'screen_name': u'joshua_landis', u'name': u'Joshua Landis'}], 'coordinates': None, 'id_str': u'755171545736249344', 'retweets': 0, 'user_friend': 1617, 'text_clean': 'joshualandis true syr coastal cities sunnixtian syrian town shared sunnis amp alawis first fr census https', 'user_follower': 421, 'user': u\"D'ARAMITZ\", 'favorites': 0, 'user_tweets': 39, 'place_country': ['no_geo'], 'ent_hashtag': [], 'text_token': [u'joshualandi', u'true', u'syr', u'coastal', u'citi', u'sunni', u'xtian', u'syrian', u'town', u'share', u'sunni', u'alawi', u'first', u'censu'], 'created_at': u'Mon Jul 18 22:45:01 +0000 2016', 'user_location': None, 'place_type': ['no_geo'], 'user_statuses': 30767, '_id': ObjectId('578d5bf1bd526e0391e0e327')}\n",
      "{'text': u'1793 1/2C Liberty Cap (Cohen-2) PCGS AU58+ CAC \\u2013 Condition\\xa0Census https://t.co/cngsUaU4Bh', 'place_countrycode': ['no_geo'], 'place_name': ['no_geo'], 'user_screen_name': u'bestcoin', 'ent_user_mention': [], 'coordinates': None, 'id_str': u'755172851045380096', 'retweets': 0, 'user_friend': 1971, 'text_clean': ' c liberty cap cohen pcgs au cac  conditioncensus ', 'user_follower': 1833, 'user': u'BestCoin.Com', 'favorites': 0, 'user_tweets': 94, 'place_country': ['no_geo'], 'ent_hashtag': [], 'text_token': [u'liberti', u'cap', u'cohen', u'pcg', u'cac', u'condit', u'censu'], 'created_at': u'Mon Jul 18 22:50:12 +0000 2016', 'user_location': u'Southern California', 'place_type': ['no_geo'], 'user_statuses': 496272, '_id': ObjectId('578d5d25bd526e0391e0e328')}\n",
      "{'text': u'1793 1/2C Liberty Cap (Cohen-2) PCGS AU58+ CAC \\u2013 Condition\\xa0Census https://t.co/IDAMDspooa', 'place_countrycode': ['no_geo'], 'place_name': ['no_geo'], 'user_screen_name': u'bestcoin', 'ent_user_mention': [], 'coordinates': None, 'id_str': u'755172855487221760', 'retweets': 0, 'user_friend': 1971, 'text_clean': ' c liberty cap cohen pcgs au cac  conditioncensus ', 'user_follower': 1833, 'user': u'BestCoin.Com', 'favorites': 0, 'user_tweets': 94, 'place_country': ['no_geo'], 'ent_hashtag': [], 'text_token': [u'liberti', u'cap', u'cohen', u'pcg', u'cac', u'condit', u'censu'], 'created_at': u'Mon Jul 18 22:50:13 +0000 2016', 'user_location': u'Southern California', 'place_type': ['no_geo'], 'user_statuses': 496273, '_id': ObjectId('578d5d26bd526e0391e0e329')}\n",
      "{'text': u'RT @sufstjames: Feel uncomfortable w the census not being anon this year particularly as prominent aussies call for deportation due to reli\\u2026', 'place_countrycode': ['no_geo'], 'place_name': ['no_geo'], 'user_screen_name': u'sjhfletcher', 'ent_user_mention': [{u'id': 331064883, u'indices': [3, 14], u'id_str': u'331064883', u'screen_name': u'sufstjames', u'name': u'Havana Dadparty'}], 'coordinates': None, 'id_str': u'755176013223428097', 'retweets': 0, 'user_friend': 220, 'text_clean': 'sufstjames feel uncomfortable w census anon year particularly prominent aussies call deportation due reli', 'user_follower': 1091, 'user': u'Sarah JH Fletcher', 'favorites': 0, 'user_tweets': 100, 'place_country': ['no_geo'], 'ent_hashtag': [], 'text_token': [u'sufstjam', u'feel', u'uncomfort', u'censu', u'anon', u'year', u'particularli', u'promin', u'aussi', u'call', u'deport', u'due', u'reli'], 'created_at': u'Mon Jul 18 23:02:46 +0000 2016', 'user_location': u'Australia', 'place_type': ['no_geo'], 'user_statuses': 37349, '_id': ObjectId('578d6017bd526e0391e0e33b')}\n",
      "{'text': u\"RT @veritygorman: .@ABSCensus officials will visit Robinvale today ahead of next month's Census. There have been concerns about under repor\\u2026\", 'place_countrycode': ['no_geo'], 'place_name': ['no_geo'], 'user_screen_name': u'Baabau', 'ent_user_mention': [{u'id': 288610842, u'indices': [3, 16], u'id_str': u'288610842', u'screen_name': u'veritygorman', u'name': u'Verity Gorman'}, {u'id': 172532664, u'indices': [19, 29], u'id_str': u'172532664', u'screen_name': u'ABSCensus', u'name': u'Census Australia'}], 'coordinates': None, 'id_str': u'755176391730040832', 'retweets': 0, 'user_friend': 1959, 'text_clean': 'veritygorman officials visit robinvale today ahead next months census concerns repor', 'user_follower': 1961, 'user': u'Shane Roberts', 'favorites': 0, 'user_tweets': 160, 'place_country': ['no_geo'], 'ent_hashtag': [], 'text_token': [u'veritygorman', u'abscensu', u'offici', u'visit', u'robinval', u'today', u'ahead', u'next', u'month', u'censu', u'concern', u'repor'], 'created_at': u'Mon Jul 18 23:04:16 +0000 2016', 'user_location': u'Hopetoun, Victoria', 'place_type': ['no_geo'], 'user_statuses': 79620, '_id': ObjectId('578d6071bd526e0391e0e33c')}\n",
      "{'text': u'We knew all along job reports werent right, now we know why: Census *FAKED* 2012 Election Jobs Report https://t.co/PsGOdwy2SY #tcot #PJNet', 'place_countrycode': ['no_geo'], 'place_name': ['no_geo'], 'user_screen_name': u'WSCP2', 'ent_user_mention': [], 'coordinates': None, 'id_str': u'755176673591504896', 'retweets': 0, 'user_friend': 2806, 'text_clean': 'knew along job reports werent right know census faked  election jobs report  tcot pjnet', 'user_follower': 3957, 'user': u'WSCP2', 'favorites': 0, 'user_tweets': 140, 'place_country': ['no_geo'], 'ent_hashtag': [{u'indices': [126, 131], u'text': u'tcot'}, {u'indices': [132, 138], u'text': u'PJNet'}], 'text_token': [u'knew', u'along', u'job', u'report', u'werent', u'right', u'know', u'censu', u'fake', u'elect', u'job', u'report', u'tcot', u'pjnet'], 'created_at': u'Mon Jul 18 23:05:24 +0000 2016', 'user_location': u'http://patriotjournalist.com', 'place_type': ['no_geo'], 'user_statuses': 163777, '_id': ObjectId('578d60b4bd526e0391e0e33d')}\n",
      "{'text': u'RT @PohleRichard: Struggling with a #swan during the annual #swanupping census on the #river #thames. @TimesPictures @thetimes https://t.co\\u2026', 'place_countrycode': ['no_geo'], 'place_name': ['no_geo'], 'user_screen_name': u'glosphotog', 'ent_user_mention': [{u'id': 3253534606, u'indices': [3, 16], u'id_str': u'3253534606', u'screen_name': u'PohleRichard', u'name': u'Richard Pohle'}, {u'id': 704842044, u'indices': [102, 116], u'id_str': u'704842044', u'screen_name': u'TimesPictures', u'name': u'The Times Pictures'}, {u'id': 6107422, u'indices': [117, 126], u'id_str': u'6107422', u'screen_name': u'thetimes', u'name': u'The Times of London'}], 'coordinates': None, 'id_str': u'755176735788953600', 'retweets': 0, 'user_friend': 1961, 'text_clean': 'pohlerichard struggling swan annual swanupping census river thames ', 'user_follower': 1400, 'user': u'Michael Smith', 'favorites': 0, 'user_tweets': 28, 'place_country': ['no_geo'], 'ent_hashtag': [{u'indices': [36, 41], u'text': u'swan'}, {u'indices': [60, 71], u'text': u'swanupping'}, {u'indices': [86, 92], u'text': u'river'}, {u'indices': [93, 100], u'text': u'thames'}], 'text_token': [u'pohlerichard', u'struggl', u'swan', u'annual', u'swanup', u'censu', u'river', u'thame', u'timespictur', u'thetim'], 'created_at': u'Mon Jul 18 23:05:38 +0000 2016', 'user_location': u'Cheltenham', 'place_type': ['no_geo'], 'user_statuses': 1549, '_id': ObjectId('578d60c3bd526e0391e0e33e')}\n",
      "{'text': u'RT @mikelayestaran: Todos con las purgas de #Erdogan a vueltas, y \\xbfEgipto? lleva purgando con el visto bueno de Occidente desde 2013 https:\\u2026', 'place_countrycode': ['no_geo'], 'place_name': ['no_geo'], 'user_screen_name': u'miguellaws1', 'ent_user_mention': [{u'id': 50908485, u'indices': [3, 18], u'id_str': u'50908485', u'screen_name': u'mikelayestaran', u'name': u'mikel ayestaran'}], 'coordinates': None, 'id_str': u'755176748388651008', 'retweets': 0, 'user_friend': 197, 'text_clean': 'mikelayestaran todos con las purgas de erdogan vueltas egipto lleva purgando con el visto bueno de occidente desde  https', 'user_follower': 50, 'user': u'miguellaws', 'favorites': 0, 'user_tweets': 1, 'place_country': ['no_geo'], 'ent_hashtag': [{u'indices': [44, 52], u'text': u'Erdogan'}], 'text_token': [u'mikelayestaran', u'todo', u'con', u'purga', u'erdogan', u'vuelta', u'egipto', u'lleva', u'purgando', u'con', u'visto', u'bueno', u'occident', u'desd'], 'created_at': u'Mon Jul 18 23:05:41 +0000 2016', 'user_location': None, 'place_type': ['no_geo'], 'user_statuses': 3591, '_id': ObjectId('578d60c6bd526e0391e0e33f')}\n",
      "\n",
      "New Census Tweets Collection Size: 1399\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pymongo.results.UpdateResult at 0x10453aeb0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tweepy\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "from pymongo import MongoClient \n",
    "import json\n",
    "import re\n",
    "from aylienapiclient import textapi\n",
    "from unidecode import unidecode\n",
    "import googlemaps\n",
    "\n",
    "\n",
    "#text analytics imports\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "\n",
    "PUNCTUATION = set(string.punctuation)\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "STEMMER = PorterStemmer()\n",
    "LEMMER = WordNetLemmatizer()\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "\n",
    "\n",
    "#Twitter API credentials\n",
    "auth = tweepy.OAuthHandler('YOUR_INFO', 'YOUR_INFO')\n",
    "auth.set_access_token('YOUR_INFO', 'YOUR_INFO')\n",
    "\n",
    "consumer_key = 'YOUR_INFO'\n",
    "consumer_secret = 'YOUR_INFO'\n",
    "access_token = 'YOUR_INFOYOUR_INFO'\n",
    "access_token_secret = 'YOUR_INFO'\n",
    "\n",
    "#Google Maps API key\n",
    "gmaps = googlemaps.Client(key='YOUR_INFO')\n",
    "\n",
    "#AYLIEN credentials\n",
    "#rjshanahan\n",
    "my_aylien = textapi.Client(\"YOUR_INFO\", \"YOUR_INFO\")\n",
    "\n",
    "\n",
    "\n",
    "#MongoDB connection\n",
    "client = MongoClient('YOUR_INFO')\n",
    "db = client.twitter01\n",
    "collection = db.tweets_census \n",
    "\n",
    "\n",
    "#regex patterns\n",
    "problemchars = re.compile(r'[\\[=\\+/&<>;:!\\\\|*^\\'\"\\?%$.@)Â°#(_\\,\\t\\r\\n0-9-â€”\\]]')\n",
    "url_finder = re.compile(r'http[s]?:\\/\\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "emojis = re.compile(\"[\"\n",
    "        u\"\\U0001F600\\\\-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300\\\\-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680\\\\-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0\\\\-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "stop = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*')\n",
    "# username = re.compile(r'(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[A-Za-z0-9]+)')\n",
    "username = re.compile(r'(@)\\w+( )')\n",
    "retweeted = re.compile(r'^rt ')\n",
    "# hashtag = re.compile(r'#(\\w+)')\n",
    "reempty = re.compile(r'^$|\\s+')\n",
    "\n",
    "\n",
    "#function to flatten nested dictionaries in tweet JSON object\n",
    "def flatten(indict, current_key=None, outerdict=None):\n",
    "    if outerdict is None:\n",
    "        outerdict = {}\n",
    "    for key, value in indict.items():\n",
    "        newkey = current_key + '__' + key if current_key else key\n",
    "        if type(value) is not dict:\n",
    "            outerdict[newkey] = value\n",
    "        else:\n",
    "            flatten(value, current_key=newkey, outerdict=outerdict)\n",
    "    return outerdict\n",
    "\n",
    "\n",
    "\n",
    "#function to lookup and count occurences of specific words in Tweet body\n",
    "def tweet_cleaner(bodyText):\n",
    "    \n",
    "    #append nltk libraries\n",
    "    nltk.data.path.append(\"/Users/rjshanahan/nltk_data\")\n",
    "    \n",
    "    \n",
    "    tokens = tweet_tokenizer.tokenize(bodyText)\n",
    "    lowercased = [t.lower() for t in tokens]\n",
    "    no_punctuation = []\n",
    "    for word in lowercased:\n",
    "        punct_removed = ''.join([letter for letter in word if not letter in PUNCTUATION and not letter.isdigit()])\n",
    "        no_punctuation.append(punct_removed)\n",
    "    no_stopwords = [w for w in no_punctuation if not w in STOPWORDS]\n",
    "    stemmed = [STEMMER.stem(w) for w in no_stopwords]\n",
    "    lemmed = [LEMMER.lemmatize(w) for w in stemmed]\n",
    "    no_links = [w for w in lemmed if (not 'http' in w) and len(w)>2]\n",
    "            \n",
    "    return no_links\n",
    "\n",
    "\n",
    "def transform_tweet(line):\n",
    "    return re.compile('#\\w+ ').sub('', re.compile('RT @\\w+: ').sub('', line, count=1)).strip()\n",
    "\n",
    "\n",
    "\n",
    "#define class for streaming Twitter data\n",
    "class StdOutListener(StreamListener):\n",
    "\n",
    "    def on_data(self, tweet_data):   \n",
    "\n",
    "        #define objects: https://dev.twitter.com/overview/api/tweets\n",
    "        tweet = json.loads(tweet_data)\n",
    "        tweet = flatten(tweet)\n",
    "\n",
    "        #elements of interest\n",
    "        id_str = tweet[\"id_str\"]\n",
    "        created_at = tweet[\"created_at\"]\n",
    "        id_str = tweet[\"id_str\"]\n",
    "#        text = tweet[\"text\"].encode('ascii', 'ignore')\n",
    "#         text = unidecode(tweet[\"text\"])\n",
    "        text = tweet[\"text\"]\n",
    "#         text_sentiment = my_aylien.Sentiment(text)\n",
    "        text_token = tweet_cleaner(text)\n",
    "        text_clean = retweeted.sub('', stop.sub('', problemchars.sub('', emojis.sub('', url_finder.sub('', username.sub('', text.encode('ascii', 'ignore').lower().strip()))))))\n",
    "#         text_clean = stop.sub('', problemchars.sub('', url_finder.sub('', username.sub('', text.encode('utf-8').lower().strip()))))\n",
    "        coord = tweet[\"coordinates\"]\n",
    "        fav = tweet[\"favorite_count\"]\n",
    "        rtwt = tweet[\"retweet_count\"]\n",
    "        user = tweet[\"user__name\"]\n",
    "        user_follower = tweet[\"user__followers_count\"]\n",
    "        user_friend = tweet[\"user__friends_count\"]\n",
    "        user_tweets = tweet[\"user__listed_count\"]\n",
    "        user_location = tweet[\"user__location\"]\n",
    "        user_statuses = tweet[\"user__statuses_count\"]\n",
    "        user_screen_name = tweet[\"user__screen_name\"]\n",
    "        ent_hashtag = tweet[\"entities__hashtags\"]\n",
    "        ent_user_mention = tweet[\"entities__user_mentions\"]\n",
    "        place_country = [tweet[\"place__country\"] if \"place__country\" in tweet else \"no_geo\"]\n",
    "        place_countrycode = [tweet[\"place__country_code\"] if \"place__country_code\" in tweet else \"no_geo\"]\n",
    "        place_name = [tweet[\"place__name\"] if \"place__name\" in tweet else \"no_geo\"]\n",
    "        place_type = [tweet[\"place__place_type\"] if \"place__place_type\" in tweet else \"no_geo\"]\n",
    "        \n",
    "        \n",
    "        #create dict to insert into MongoDB\n",
    "        obj = { \n",
    "            \"id_str\":id_str,\n",
    "            \"created_at\":created_at,\n",
    "            \"id_str\":id_str,\n",
    "            \"text\":text,\n",
    "#             \"text_sentiment\":text_sentiment,\n",
    "            \"text_token\":text_token,\n",
    "            \"text_clean\":text_clean,\n",
    "            \"coordinates\":coord,\n",
    "            \"favorites\":fav,\n",
    "            \"retweets\":rtwt,\n",
    "            \"user\":user,\n",
    "            \"user_follower\":user_follower,\n",
    "            \"user_friend\":user_friend,\n",
    "            \"user_tweets\":user_tweets,\n",
    "            \"user_location\":user_location,\n",
    "            \"user_statuses\":user_statuses,\n",
    "            \"user_screen_name\":user_screen_name,\n",
    "            \"ent_hashtag\":ent_hashtag,\n",
    "            \"ent_user_mention\":ent_user_mention,\n",
    "            \"place_country\":place_country,\n",
    "            \"place_countrycode\":place_countrycode,\n",
    "            \"place_name\":place_name,\n",
    "            \"place_type\":place_type\n",
    "              }\n",
    "        \n",
    "        #insert into MongoDB\n",
    "        tweetind = collection.insert_one(obj).inserted_id\n",
    "\n",
    "        print(obj)\n",
    "\n",
    "        return True\n",
    "\n",
    "    def on_error(self, status):\n",
    "\n",
    "        #error 420 = API throttling - too many connections usually\n",
    "        print(status)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#filter for #MyCensus\n",
    "def streamer(consumer_key, consumer_secret, access_token, access_token_secret):\n",
    "    \n",
    "    #This handles Twitter authetification and the connection to Twitter Streaming AP\n",
    "    if __name__ == '__main__':\n",
    "    \n",
    "        l = StdOutListener()\n",
    "    \n",
    "    #Twitter API access with streaming class\n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "    stream = Stream(auth, l)\n",
    "    \n",
    "    try:\n",
    "        stream.filter(track=['census'])\n",
    "    except:  \n",
    "        stream.disconnect()\n",
    "        \n",
    "      \n",
    "\n",
    "        \n",
    "#call streamer function\n",
    "print('Existing Census Tweets Collection Size: ' + str(db.tweets_census.count()) + '\\n')\n",
    "streamer(consumer_key, consumer_secret, access_token, access_token_secret)\n",
    "print('\\nNew Census Tweets Collection Size: ' + str(db.tweets_census.count()) + '\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# SENTIMENT ANALYSIS\n",
    "#add new empty field\n",
    "db.tweets_census.update_many({\"text_sentiment\":{\"$exists\" : 0}},\n",
    "                        {\"$set\" : {\"text_sentiment\":1}},\n",
    "                        False)\n",
    "\n",
    "    \n",
    "#initialise bulk UPDATE process for \"text_sentiment\" field\n",
    "bulk = db.tweets_census.initialize_ordered_bulk_op()\n",
    "counter = 0\n",
    "\n",
    "for record in db.tweets_census.find( \n",
    "                      {\"text\":{\"$exists\" : 1},\n",
    "                      \"text_sentiment\":{\"$eq\" : 1}},                #'where' clause\n",
    "                      {\"text\":1,\n",
    "                       \"_id\":1},\n",
    "                modifiers={\"$snapshot\": True}):                     #\"snapshot\" means the _id ordering is kept\n",
    "    \n",
    "    my_text = record[\"text\"]\n",
    "    \n",
    "    my_sentiment = my_aylien.Sentiment(my_text.encode('utf-8'))     #call the AYLIEN API\n",
    "    \n",
    "    # now process in bulk\n",
    "    # calc value first\n",
    "    bulk.find({ '_id': record['_id'] }).update({ '$set': { 'text_sentiment': my_sentiment } })\n",
    "    counter =+ 1\n",
    "    \n",
    "    if ( counter % 1000 == 0 ):\n",
    "        bulk.execute()\n",
    "        bulk = db.tweets_census.initialize_ordered_bulk_op()\n",
    "\n",
    "if ( counter % 1000 != 0 ):\n",
    "    bulk.execute()\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#GEOCODE PLACE and USER_LOCATION NAME VIA GOOGLE MAPS API\n",
    "#function to geocode PLACE_NAME\n",
    "def lat_lng_finder(gmaps, place_name):\n",
    "        \n",
    "    try:\n",
    "        geocode_result = gmaps.geocode(place_name)\n",
    "    \n",
    "        loc_dict = {\n",
    "            'latitude': geocode_result[0]['geometry']['location']['lat'],\n",
    "            'longitude': geocode_result[0]['geometry']['location']['lng']\n",
    "                }\n",
    "    except:\n",
    "        loc_dict = {\n",
    "            'latitude': \"\",\n",
    "            'longitude': \"\"\n",
    "                }\n",
    "    \n",
    "    return loc_dict\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#add new empty field - set to '1' to make it identifiable\n",
    "db.tweets_census.update_many({\n",
    "#         \"lat_lon\" : {\"$exists\" : 0},\n",
    "                              \"lat_lon_loc\" : {\"$exists\" : 0}\n",
    "                             },\n",
    "                        {\n",
    "#         \"$set\" : {\"lat_lon\":1},\n",
    "                         \"$set\" : {\"lat_lon_loc\":1}\n",
    "                        },\n",
    "                        False)\n",
    "\n",
    "    \n",
    "#initialise bulk UPDATE process for \"text_sentiment\" field\n",
    "bulk = db.tweets_census.initialize_ordered_bulk_op()\n",
    "counter = 0\n",
    "    \n",
    "\n",
    "for record in db.tweets_census.find( \n",
    "                    { \"user_location\" : {\"$exists\" : 1},\n",
    "                      \"user_location\" : {\"$ne\" : \"None\"},\n",
    "                      \"user_location\" : {\"$ne\" : 1},\n",
    "#                       \"place_name\" : {\"$exists\" : 1},\n",
    "#                       \"place_name\": {\"$ne\" : \"no_geo\"},\n",
    "#                       \"lat_lon\":{\"$eq\" : 1},\n",
    "                      \"lat_lon_loc\":{\"$eq\" : 1}\n",
    "                    },                #'where' clause\n",
    "                        {\"user_location\" : 1,\n",
    "#                          \"place_name\" : 1,\n",
    "                         \"_id\" : 1},\n",
    "                modifiers={\"$snapshot\": True}):                     #\"snapshot\" means the _id ordering is kept\n",
    "    \n",
    "    my_loc = record[\"user_location\"]\n",
    "#     my_place = record[\"place_name\"]\n",
    "    \n",
    "    my_lat_lon_loc = lat_lng_finder(gmaps, my_loc)\n",
    "#     my_lat_lon = lat_lng_finder(gmaps, my_place)\n",
    "    \n",
    "    # now process in bulk\n",
    "    # calc value first\n",
    "    bulk.find({ '_id': record['_id'] }).update({ '$set': { 'lat_lon_loc': my_lat_lon_loc }}),\n",
    "#                                                '$set': { 'lat_lon': my_lat_lon }})\n",
    "    counter =+ 1\n",
    "    \n",
    "    if ( counter % 1000 == 0 ):\n",
    "        bulk.execute()\n",
    "        bulk = db.tweets_census.initialize_ordered_bulk_op()\n",
    "\n",
    "if ( counter % 1000 != 0 ):\n",
    "    bulk.execute()\n",
    "\n",
    "#set '1' fields to NULL for those that weren't coded\n",
    "db.tweets_census.update_many({\"lat_lon_loc\":{\"$eq\":1}},\n",
    "#                               \"lat_lon\":{\"$eq\":1}},\n",
    "#                         {\"$set\" : {\"lat_lon\":\"null\"},\n",
    "                        {\"$set\" : {\"lat_lon_loc\":\"null\"}},\n",
    "                        False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Additional Fields against Streamed Twitter data\n",
    "- adds 'clean' text field\n",
    "- then does sentiment analysis\n",
    "- then geocodes tweets based on user's location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEXT CLEANER\n",
    "#add new empty field\n",
    "db.tweets_census.update_many({\"text_clean\":{\"$exists\" : 0}},\n",
    "                        {\"$set\" : {\"text_clean\":1}},\n",
    "                        False)\n",
    "\n",
    "    \n",
    "#initialise bulk UPDATE process for \"text_sentiment\" field\n",
    "bulk = db.tweets_census.initialize_ordered_bulk_op()\n",
    "counter = 0\n",
    "\n",
    "for record in db.tweets_census.find( \n",
    "                      {\"text\":{\"$exists\" : 1},\n",
    "                      \"text_clean\":{\"$eq\" : 1}},                #'where' clause\n",
    "                      {\"text\":1,\n",
    "                       \"_id\":1},\n",
    "                modifiers={\"$snapshot\": True}):                     #\"snapshot\" means the _id ordering is kept\n",
    "    \n",
    "    my_text = record[\"text\"]    \n",
    "    \n",
    "    my_text_clean = retweeted.sub('', stop.sub('', problemchars.sub('', emojis.sub('', url_finder.sub('', username.sub('', my_text.encode('ascii', 'ignore').lower().strip()))))))\n",
    "    \n",
    "    \n",
    "    # now process in bulk\n",
    "    # calc value first\n",
    "    bulk.find({ '_id': record['_id'] }).update({ '$set': { 'text_clean': my_text_clean } })\n",
    "    counter =+ 1\n",
    "    \n",
    "    if ( counter % 1000 == 0 ):\n",
    "        bulk.execute()\n",
    "        bulk = db.tweets_census.initialize_ordered_bulk_op()\n",
    "\n",
    "if ( counter % 1000 != 0 ):\n",
    "    bulk.execute()\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# SENTIMENT ANALYSIS\n",
    "#add new empty field\n",
    "db.tweets_census.update_many({\"text_sentiment\":{\"$exists\" : 0}},\n",
    "                        {\"$set\" : {\"text_sentiment\":1}},\n",
    "                        False)\n",
    "\n",
    "    \n",
    "#initialise bulk UPDATE process for \"text_sentiment\" field\n",
    "bulk = db.tweets_census.initialize_ordered_bulk_op()\n",
    "counter = 0\n",
    "\n",
    "for record in db.tweets_census.find( \n",
    "                      {\"text\":{\"$exists\" : 1},\n",
    "                      \"text_sentiment\":{\"$eq\" : 1}},                #'where' clause\n",
    "                      {\"text\":1,\n",
    "                       \"_id\":1},\n",
    "                modifiers={\"$snapshot\": True}):                     #\"snapshot\" means the _id ordering is kept\n",
    "    \n",
    "    my_text = record[\"text\"]\n",
    "    \n",
    "    my_sentiment = my_aylien.Sentiment(my_text.encode('utf-8'))     #call the AYLIEN API\n",
    "    \n",
    "    # now process in bulk\n",
    "    # calc value first\n",
    "    bulk.find({ '_id': record['_id'] }).update({ '$set': { 'text_sentiment': my_sentiment } })\n",
    "    counter =+ 1\n",
    "    \n",
    "    if ( counter % 1000 == 0 ):\n",
    "        bulk.execute()\n",
    "        bulk = db.tweets_census.initialize_ordered_bulk_op()\n",
    "\n",
    "if ( counter % 1000 != 0 ):\n",
    "    bulk.execute()\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#GEOCODE PLACE and USER_LOCATION NAME VIA GOOGLE MAPS API\n",
    "#function to geocode PLACE_NAME\n",
    "def lat_lng_finder(gmaps, place_name):\n",
    "        \n",
    "    try:\n",
    "        geocode_result = gmaps.geocode(place_name)\n",
    "    \n",
    "        loc_dict = {\n",
    "            'latitude': geocode_result[0]['geometry']['location']['lat'],\n",
    "            'longitude': geocode_result[0]['geometry']['location']['lng']\n",
    "                }\n",
    "    except:\n",
    "        loc_dict = {\n",
    "            'latitude': \"\",\n",
    "            'longitude': \"\"\n",
    "                }\n",
    "    \n",
    "    return loc_dict\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#add new empty field - set to '1' to make it identifiable\n",
    "db.tweets_census.update_many({\n",
    "#         \"lat_lon\" : {\"$exists\" : 0},\n",
    "                              \"lat_lon_loc\" : {\"$exists\" : 0}\n",
    "                             },\n",
    "                        {\n",
    "#         \"$set\" : {\"lat_lon\":1},\n",
    "                         \"$set\" : {\"lat_lon_loc\":1}\n",
    "                        },\n",
    "                        False)\n",
    "\n",
    "    \n",
    "#initialise bulk UPDATE process for \"text_sentiment\" field\n",
    "bulk = db.tweets_census.initialize_ordered_bulk_op()\n",
    "counter = 0\n",
    "    \n",
    "\n",
    "for record in db.tweets_census.find( \n",
    "                    { \"user_location\" : {\"$exists\" : 1},\n",
    "                      \"user_location\" : {\"$ne\" : \"None\"},\n",
    "                      \"user_location\" : {\"$ne\" : 1},\n",
    "#                       \"place_name\" : {\"$exists\" : 1},\n",
    "#                       \"place_name\": {\"$ne\" : \"no_geo\"},\n",
    "#                       \"lat_lon\":{\"$eq\" : 1},\n",
    "                      \"lat_lon_loc\":{\"$eq\" : 1}\n",
    "                    },                #'where' clause\n",
    "                        {\"user_location\" : 1,\n",
    "#                          \"place_name\" : 1,\n",
    "                         \"_id\" : 1},\n",
    "                modifiers={\"$snapshot\": True}):                     #\"snapshot\" means the _id ordering is kept\n",
    "    \n",
    "    my_loc = record[\"user_location\"]\n",
    "#     my_place = record[\"place_name\"]\n",
    "    \n",
    "    my_lat_lon_loc = lat_lng_finder(gmaps, my_loc)\n",
    "#     my_lat_lon = lat_lng_finder(gmaps, my_place)\n",
    "    \n",
    "    # now process in bulk\n",
    "    # calc value first\n",
    "    bulk.find({ '_id': record['_id'] }).update({ '$set': { 'lat_lon_loc': my_lat_lon_loc }}),\n",
    "#                                                '$set': { 'lat_lon': my_lat_lon }})\n",
    "    counter =+ 1\n",
    "    \n",
    "    if ( counter % 1000 == 0 ):\n",
    "        bulk.execute()\n",
    "        bulk = db.tweets_census.initialize_ordered_bulk_op()\n",
    "\n",
    "if ( counter % 1000 != 0 ):\n",
    "    bulk.execute()\n",
    "\n",
    "#set '1' fields to NULL for those that weren't coded\n",
    "db.tweets_census.update_many({\"lat_lon_loc\":{\"$eq\":1}},\n",
    "#                               \"lat_lon\":{\"$eq\":1}},\n",
    "#                         {\"$set\" : {\"lat_lon\":\"null\"},\n",
    "                        {\"$set\" : {\"lat_lon_loc\":\"null\"}},\n",
    "                        False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3092"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.tweets_census.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emoji Analysis\n",
    "- http://stats.seandolinar.com/emoji-utf-8-and-python/\n",
    "- https://github.com/seandolinar/socialmediaparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emoji</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>ðŸ˜‚</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>ðŸ‘‡</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>ðŸ˜Œ</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>ðŸ˜¼</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>ðŸ˜</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917</th>\n",
       "      <td>ðŸŽ‰</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>â¤ï¸</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>ðŸ’¯</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>ðŸ˜Š</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>ðŸ˜˜</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>ðŸ˜­</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>ðŸ˜–</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>ðŸ’¦</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>ðŸ˜©</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>ðŸ™Š</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>ðŸ“±</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1127</th>\n",
       "      <td>âœ‹</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>ðŸ˜´</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>ðŸ™ˆ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>ðŸˆ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     emoji  count\n",
       "535     ðŸ˜‚     20\n",
       "50      ðŸ‘‡      9\n",
       "537     ðŸ˜Œ      6\n",
       "489     ðŸ˜¼      4\n",
       "536     ðŸ˜      4\n",
       "917     ðŸŽ‰      4\n",
       "675     â¤ï¸      3\n",
       "214     ðŸ’¯      3\n",
       "543     ðŸ˜Š      2\n",
       "525     ðŸ˜˜      2\n",
       "504     ðŸ˜­      2\n",
       "515     ðŸ˜–      2\n",
       "207     ðŸ’¦      2\n",
       "508     ðŸ˜©      2\n",
       "479     ðŸ™Š      1\n",
       "130     ðŸ“±      1\n",
       "1127     âœ‹      1\n",
       "481     ðŸ˜´      1\n",
       "477     ðŸ™ˆ      1\n",
       "123     ðŸˆ      1"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymongo\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "\n",
    "#loads emoji key\n",
    "##os.chdir('##directory##')##\n",
    "emoji_key = pd.read_csv('emoji_table.txt', encoding='utf-8', index_col=0)\n",
    "\n",
    "# emoji_key.head(20)\n",
    "\n",
    "\n",
    "#intialize emoji count\n",
    "emoji_key['count'] = 0\n",
    "\n",
    "\n",
    "emoji_dict = emoji_key['count'].to_dict()\n",
    "\n",
    "\n",
    "emoji_list = []\n",
    "tweet_list = collection.find({'text' : { '$exists' : 1 }} )            \n",
    "for tweet in tweet_list:\n",
    "    for emoji in emoji_dict.keys():\n",
    "#         if emoji in tweet['text'] and tweet['text'][:2] != 'RT' :\n",
    "        if emoji in tweet['text']:\n",
    "            emoji_list.append(tweet['text'])\n",
    "#             print(tweet['text'])\n",
    "            emoji_dict[emoji] += 1\n",
    "            \n",
    "\n",
    "\n",
    "#prints number of unique number of tweets with emoji in them                      \n",
    "# print(len(set(emoji_list)))\n",
    "\n",
    "#print emoji_dict\n",
    "# print(tweet_list.count())\n",
    "\n",
    "#creates and sorts a data frame of a count of emoji\n",
    "emoji_count = pd.DataFrame(emoji_dict.items(), columns=['emoji', 'count'])\n",
    "emoji_count.sort_values(by=\"count\", ascending=False).head(20)\n",
    "\n",
    "#writes output file\n",
    "# with open('emoji_out.csv', 'w') as f: \n",
    "#     emoji_count.to_csv(f, sep=',', index = False, encoding='utf-8') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bulk Update specific field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#add new empty field\n",
    "# db.tweets_census.update_many({\"text_clean\":{\"$exists\" : 0}},\n",
    "db.tweets_census.update_many({},\n",
    "                        {\"$set\" : {\"text_clean\":1}},\n",
    "                        False)\n",
    "\n",
    "\n",
    "# db.tweets_census.update_many({\"lat_lon\":{\"$eq\":1}},\n",
    "#                         {\"$set\" : {\"lat_lon\":\"null\"}},\n",
    "#                         False)\n",
    "\n",
    "    \n",
    "#initialise bulk UPDATE process for \"text_sentiment\" field\n",
    "bulk = db.tweets_census.initialize_ordered_bulk_op()\n",
    "counter = 0\n",
    "\n",
    "for record in db.tweets_census.find( \n",
    "                      {\"text\":{\"$exists\" : 1},\n",
    "                      \"text_clean\":{\"$eq\" : 1}},                #'where' clause\n",
    "                      {\"text\":1,\n",
    "                       \"_id\":1},\n",
    "                modifiers={\"$snapshot\": True}):                     #\"snapshot\" means the _id ordering is kept\n",
    "    \n",
    "    my_text = record[\"text\"]\n",
    "    \n",
    "    my_text_clean = stop.sub('', problemchars.sub('', url_finder.sub('', username.sub('', my_text.encode('ascii', 'ignore').lower().strip()))))  #call the AYLIEN API\n",
    "    \n",
    "    # now process in bulk\n",
    "    # calc value first\n",
    "    bulk.find({ '_id': record['_id'] }).update({ '$set': { 'text_clean': my_text_clean } })\n",
    "    counter =+ 1\n",
    "    \n",
    "    if ( counter % 1000 == 0 ):\n",
    "        bulk.execute()\n",
    "        bulk = db.tweets_census.initialize_ordered_bulk_op()\n",
    "\n",
    "if ( counter % 1000 != 0 ):\n",
    "    bulk.execute()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN Queries and Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import pymongo\n",
    "import pprint as pp\n",
    "    \n",
    "    \n",
    "#mongodb connection function\n",
    "def get_db(db_name, coll_name):\n",
    "    from pymongo import MongoClient\n",
    "    \n",
    "    #MongoDB connection\n",
    "    client = MongoClient('YOUR_INFO')\n",
    "    db = client[db_name]\n",
    "    coll = db[coll_name]\n",
    "    \n",
    "    return db\n",
    "\n",
    "\n",
    "\n",
    "#function to capture aggregation framework query\n",
    "def make_pipeline():\n",
    "  \n",
    "    pipeline =              [\n",
    "                                  {\"$group\": {\"_id\" : '$user_screen_name',\n",
    "                                               \"count\" : {\"$sum\" : 1}}},\n",
    "                                  {\"$sort\" : {\"count\" : -1}},\n",
    "                                  {\"$limit\" : 20}\n",
    "                            ]\n",
    "                        \n",
    "        \n",
    "    return pipeline\n",
    "\n",
    "\n",
    "#function to call aggregation query\n",
    "def aggregator(db, pipeline):\n",
    "    result = db.tweets_census.aggregate(pipeline)\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "#CALL AGGREGATOR\n",
    "# if __name__ == '__main__':\n",
    "    \n",
    "#     db = get_db('twitter01', 'tweets_census')\n",
    "#     pipeline = make_pipeline()\n",
    "#     result = aggregator(db, pipeline)\n",
    "    \n",
    "#     pp.pprint(list(result))\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "#CALL QUERY\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    db = get_db('twitter01', 'tweets_census')\n",
    "        \n",
    "    result = db.tweets_census.find(\n",
    "                    { \"user_location\" : {\"$exists\" : 1},\n",
    "                      \"user_location\": {\"$ne\" : \"None\"},  \n",
    "                      \"lat_lon_loc\" : {\"$exists\" : 1},\n",
    "                    },                #'where' clause\n",
    "                        {\"user_location\" : 1,\n",
    "                         \"lat_lon_loc\" : 1,\n",
    "                         \"_id\" : 0},\n",
    "                                     )\n",
    "                                     \n",
    "#                                   {\"text_sentiment\":{\"$exists\" : 1}} ,                #'where' clause\n",
    "#                                   {\"text_sentiment.polarity\":1,    \n",
    "#                                    \"text_sentiment.polarity_confidence\":1,  #'select' clause - \"_id\":0 remove id field\n",
    "#                                    \"text_sentiment.subjectivity\":1,\n",
    "#                                    \"text_sentiment.subjectivity_confidence\":1,\n",
    "#                                    \"_id\":0}\n",
    "                                    \n",
    "        \n",
    "    for t in result:\n",
    "#         pp.pprint(t['text_sentiment']['polarity'])\n",
    "#         pp.pprint(t['user_location'])\n",
    "        pp.pprint(t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "    \n",
    "#query only\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    db = get_db('twitter01', 'tweets_census')\n",
    "        \n",
    "    result = db.tweets_census.find(\n",
    "                                  {\"text\":{\"$ne\" : \"null\"}} ,                #'where' clause\n",
    "                                  {\"text\":1,                                  #'select' clause - \"_id\":0 remove id field\n",
    "                                   \"_id\":0})\n",
    "\n",
    "#check to see if text sentiment field is null    \n",
    "# result = db.tweets_census.find(\n",
    "#                                     {\"text_sentiment\":{\"$eq\" : \"null\"} },                #'where' clause\n",
    "#                                     {\"text_sentiment\":1,                                  #'select' clause - \"_id\":0 remove id field\n",
    "#                                        \"_id\":0}\n",
    "                                     )\n",
    "\n",
    "#return fields where text sentiment fields exists\n",
    "#     result = db.tweets_census.find(\n",
    "#                                   {\"text_sentiment\":{\"$exists\" : 1}} ,                #'where' clause\n",
    "#                                   {\"text_sentiment\":1,                                  #'select' clause - \"_id\":0 remove id field\n",
    "#                                    \"_id\":0})\n",
    "\n",
    "#retrun contents of text sentiment fields\n",
    "#     result = db.tweets_census.find(\n",
    "#                                   {\"text_sentiment\":{\"$exists\" : 1}} ,                #'where' clause\n",
    "#                                   {\"text_sentiment.polarity\":1,    \n",
    "#                                    \"text_sentiment.polarity_confidence\":1,  #'select' clause - \"_id\":0 remove id field\n",
    "#                                    \"text_sentiment.subjectivity\":1,\n",
    "#                                    \"text_sentiment.subjectivity_confidence\":1,\n",
    "#                                    \"_id\":0})\n",
    "    \n",
    "    \n",
    "#     result = db.tweets_census.find(\n",
    "#                                   {\"text_sentiment\":{\"$exists\" : 1}} ,                #'where' clause\n",
    "#                                   {\"text_sentiment\":1,\n",
    "#                                    \"_id\":0})\n",
    "\n",
    "    #build list for text analysis\n",
    "#     tweet_text = []\n",
    "    \n",
    "    for t in result:\n",
    "#         tweet_text.append(t[\"text\"])\n",
    "        pp.pprint(t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregation Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## LESSON 5\n",
    "from pymongo import MongoClient\n",
    "\n",
    "#aggregate uses piping operator ie stages\n",
    "\n",
    "\n",
    "def tweet_text():\n",
    "    result = db.tweets_census.aggregate([\n",
    "                                  { \"$match\": {\"_id\" : '$user_screen_name',\n",
    "                                               \"count\" : {\"$sum\" : 1}}},\n",
    "                                  {\"$sort\" : {\"count\" : -1}},\n",
    "                                  {\"$limit\" : 10}\n",
    "                                ])\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def most_tweet():\n",
    "    result = db.tweets_census.aggregate([\n",
    "                                  { \"$group\": {\"_id\" : '$user_screen_name',\n",
    "                                               \"count\" : {\"$sum\" : 1}}},\n",
    "                                  {\"$sort\" : {\"count\" : -1}},\n",
    "                                  {\"$limit\" : 10}\n",
    "                                ])\n",
    "    return result\n",
    "                                               \n",
    "    \n",
    "def highest_ratio():\n",
    "    result = db.tweets_census.aggregate([\n",
    "                                  {\"$match\" : { \"user_friend\": {\"$gt\" : 0},\n",
    "                                               \"user_follower\": {\"$gt\" : 0}}},\n",
    "                                  {\"$project\" : {\"ratio\" : {\"$divide\" :[\"$user_follower\",\n",
    "                                                                        \"$user_friend\"]},\n",
    "                                                 \"screen_name\": \"$user_screen_name\"}},\n",
    "                                  {\"$sort\": {\"ratio\": -1}},\n",
    "                                  {\"$limit\": 1}\n",
    "                                ])\n",
    "\n",
    "    \n",
    "def unique_hashtags_byuser():\n",
    "    result = db.tweets_census.aggregate([\n",
    "                                  {\"$unwind\" : \"$ent_hashtag\"},\n",
    "                                  {\"$group\" : {\"_id\" : \"$user_screen_name\",\n",
    "                                               \"unique_hashtags\": {\n",
    "                                                                   \"$addToSet\":\"$ent_hashtag.text\"}}},\n",
    "                                  {\"$sort\": {\"_id\": -1}}\n",
    "                                ])\n",
    "\n",
    "def user_mentions():\n",
    "    result = db.tweets_census.aggregate([\n",
    "                                  {\"$unwind\" : \"$ent_user_mention\"},\n",
    "                                  {\"$group\" : {\"_id\" : \"$screen_name\",\n",
    "                                               \"count\": {\"$sum\":1}}},\n",
    "                                  {\"$sort\": {\"count\": -1}},\n",
    "                                  {\"$limit\": 1}\n",
    "                                ])  \n",
    "    \n",
    " \n",
    "    \n",
    "def unique_user_mentions():\n",
    "    result = db.tweets_census.aggregate([\n",
    "                                  {\"$unwind\" : \"$ent_user_mention\"},\n",
    "                                  {\"$group\" : {\n",
    "                                               \"_id\" : \"$screen_name\",\n",
    "                                               \"mset\": {\n",
    "                                                        \"$addToSet\":\"$screen_name\"}}},\n",
    "                                  {\"$unwind\" : \"$mset\"},\n",
    "                                  {\"$group\" : {\"_id\": \"$_id\", \"count\" : {\"$sum\" : 1}}},\n",
    "                                  {\"$sort\": {\"count\": -1}},\n",
    "                                  {\"$limit\" : 10}\n",
    "                                ])    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf-idf features for NMF...\n",
      "Extracting tf features for LDA...\n",
      "Fitting the NMF model with tf-idf features,n_samples=100 and n_features=1000...\n",
      "\n",
      "Topics in NMF model:\n",
      "Topic #0:\n",
      "matter lives joevargas yiannopoulos milo simple movement destroys black statistics facts ive hirap houdinilike horse hope japan homelessness homeless holder\n",
      "Topic #1:\n",
      "jumbo tune september chhattisgarhodishaampwbengal mpparimal jharkhandk india elephants census rt hfmasynergy homelessness http htt ht housing house hours hell houdinilike\n",
      "Topic #2:\n",
      "claim harassing completing canadians canada mdroletglobaltv cdnpoli statistics census htt ht housing horse http https idea house hours houdinilike homeless\n",
      "Topic #3:\n",
      "emoji game dm im lol use needs send darkskinchris describes used tweet favorite face thing saying available yazarsn yarasky playing\n",
      "Topic #4:\n",
      "shortest links statistics pays money make houdinilike horse hope hit homelessness hours homeless house holder housing hits zozj hispanic hispanics\n",
      "Topic #5:\n",
      "rt media im map shorten need analysis sausage ill chandersuta preelection miranda devine leave kkeneally consider yes turnbull el read\n",
      "Topic #6:\n",
      "census know new data bureau bosnia national map companies questions according population scotland reference censustransformation report figures years year watch\n",
      "Topic #7:\n",
      "statistics people police source white black kill know markdice whatno tell im incidents understand dont vulnerable facts killed thats amp\n",
      "Topic #8:\n",
      "shorten ausvotes auspol johndory https amp labor women men pmwith preferred election eve homeless sausage turnbull rt electionday spent feeding\n",
      "Topic #9:\n",
      "el del das arte faltames inspiracin rioes emoji labor shortens read homeless homelessness ht housing horse houdinilike hours house hope\n",
      "()\n",
      "Fitting LDA models with tf features, n_samples=100 and n_features=1000...\n",
      "\n",
      "Topics in LDA model:\n",
      "Topic #0:\n",
      "census data algeria injuries qi brexit based chronic companies stat lankans publish dept self demographic diseases suffer product community westernsahara\n",
      "Topic #1:\n",
      "emoji im post game people bureau virado retweeting walang subs hirap play used elle forumstatistics sorry viewspost census elections rates\n",
      "Topic #2:\n",
      "number statistics live lies size newstory lords traci brazil quina amp census killed gauravcsawant jampk mere wani reduced burhan waist\n",
      "Topic #3:\n",
      "census rt uk shorten na uklangmapping eu gidhlig lets scotland sleep cycle https favorite rural countries fisheries agriculture surveys nationwide\n",
      "Topic #4:\n",
      "emoji rt amp game shorten like dm population turnbull check im head tweet playing say good growth use darkskinchris time\n",
      "Topic #5:\n",
      "didnt gutter bishop wow wtf bye shortens medicare tweeted racist donald ive trust trump deaths try cops blacks listen respond\n",
      "Topic #6:\n",
      "rt statistics census shorten people ausvotes tune jumbo september elephants india mpparimal chhattisgarhodishaampwbengal jharkhandk auspol https labor links shortest amp\n",
      "Topic #7:\n",
      "census statistics jul marketresearch rt market usa like know today population living wholesale craigieburn report night whites equipment thing stationery\n",
      "Topic #8:\n",
      "media rt sausage eating shorten billshortenmps social mini generated storm unique approach au newsaus snag bacon like hits australias ed\n",
      "Topic #9:\n",
      "statistics matter black lives simple movement yiannopoulos milo joevargas destroys college el del mathematics video elementary rock canur valley das\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient \n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "\n",
    "\n",
    "#MongoDB connection\n",
    "client = MongoClient('YOUR_INFO')\n",
    "db = client.twitter01\n",
    "collection = db.tweets_census \n",
    "\n",
    "\n",
    "# CHANGE PARAMETERS AS REQUIRED\n",
    "n_samples = 100\n",
    "n_features = 1000\n",
    "n_topics = 10\n",
    "n_top_words = 20\n",
    "\n",
    "\n",
    "\n",
    "# create sequence of strings to feed into SKlearn Topic Model\n",
    "# GSR_text = myDF_GSR.select(['eventDescription_clean']).toPandas()\n",
    "# GSR_text = list(chain.from_iterable(GSR_text.values.tolist()))\n",
    "\n",
    "text_result = db.tweets_census.find(\n",
    "                              {\"text_clean\":{\"$exists\" : 1}} ,                #'where' clause\n",
    "                              {\"text_clean\":1,\n",
    "                               \"_id\":0})\n",
    "\n",
    "#build list for text analysis\n",
    "tweet_text = []\n",
    "\n",
    "for t in text_result:\n",
    "    tweet_text.append(t[\"text_clean\"])\n",
    "    #pp.pprint(t)\n",
    "\n",
    "\n",
    "# output human readable topic content\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "    \n",
    "\n",
    "\n",
    "# create list of topic words for labelling\n",
    "def list_top_words(model, feature_names, n_top_words):\n",
    "    list_topic = []\n",
    "    \n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        list_mini_topic = []\n",
    "\n",
    "        for i in topic.argsort()[:-n_top_words - 1:-1]:\n",
    "            list_mini_topic.append(feature_names[i])\n",
    "        \n",
    "        list_topic.append(list_mini_topic)\n",
    "        \n",
    "    return list_topic\n",
    "\n",
    "\n",
    "    \n",
    "# MODEL ONE - NMF\n",
    "print(\"Extracting tf-idf features for NMF...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(input='content',\n",
    "                                   #max_df=0.95, \n",
    "                                   #min_df=0.1, \n",
    "                                   max_features=n_features,\n",
    "                                   stop_words='english')\n",
    "\n",
    "\n",
    "tfidf = tfidf_vectorizer.fit_transform(tweet_text)\n",
    "\n",
    "\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(input='content',\n",
    "                                #max_df=0.95, \n",
    "                                #min_df=2, \n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(tweet_text)\n",
    "\n",
    "print(\"Fitting the NMF model with tf-idf features,\"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "\n",
    "nmf = NMF(n_components=n_topics, \n",
    "          random_state=1, \n",
    "          alpha=.1, \n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "\n",
    "print(\"\\nTopics in NMF model:\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, \n",
    "                tfidf_feature_names, \n",
    "                n_top_words)\n",
    "\n",
    "\n",
    "# MODEL 2: LDA\n",
    "print(\"Fitting LDA models with tf features, n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "lda = LatentDirichletAllocation(n_topics=n_topics, \n",
    "                                max_iter=5,\n",
    "                                learning_method='online', \n",
    "                                learning_offset=50.,\n",
    "                                random_state=0).fit(tf)\n",
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, \n",
    "                tf_feature_names, \n",
    "                n_top_words)\n",
    "\n",
    "\n",
    "\n",
    "# build list of lists with top n topic words - used in labelling functions to follow\n",
    "topic_words = list_top_words(lda, \n",
    "                tf_feature_names, \n",
    "                n_top_words)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
